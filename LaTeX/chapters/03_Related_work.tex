% Chapter 3: Related Work
% This chapter reviews prior research on LLMs as synthetic survey respondents,
% questionnaire design effects, and demographic conditioning approaches

\chapter{Related Work}
\label{ch:related-work}

This chapter reviews the empirical literature on LLMs as synthetic survey respondents, questionnaire design effects, and demographic conditioning.

%-----------------------------------------------------------
\section{LLMs as Synthetic Survey Respondents}
\label{sec:llm-synthetic-respondents}

The systematic use of LLMs as synthetic survey respondents was introduced by \textcite{argyle2023silicon}, who coined the term ``silicon sampling.'' Their study demonstrated that GPT-3, when provided with demographic backstories drawn from real survey respondents, could produce response distributions correlating with actual political attitude data from the American National Election Studies. The model reproduced known demographic patterns, such as higher Democratic identification among Black respondents and higher Republican identification among white evangelical Christians. This work established that language models trained on vast text corpora contain implicit demographic-attitude relationships that can be elicited through appropriate prompting.

Argyle et al.\ validated their approach primarily through correlation with aggregate survey statistics. However, correlation does not capture whether synthetic data reproduces the full shape of human distributions; two distributions can correlate highly while differing substantially in variance, skewness, or modality. Distributional metrics such as Wasserstein distance provide more comprehensive assessment by quantifying differences across the entire response range.

Subsequent research has explored the boundaries of the silicon sampling approach. \textcite{sarstedt2024silicon} provide guidelines for using silicon samples in consumer and marketing research, comparing GPT-generated responses to human responses across multiple constructs. They found that while LLMs show promise for preliminary tasks like survey pretesting and pilot studies, they are not reliable substitutes for human respondents in main studies intended to draw substantive conclusions. Their recommendation is to treat silicon samples as complements rather than replacements for human data collection.

\textcite{sun2024random} demonstrated that using only group-level demographic information---without individual backstories---language models can generate response distributions with correlations above 0.8 compared to actual U.S.\ polling data for many political questions. However, performance varied substantially by topic: questions about frequently polled attitudes showed stronger correspondence, while less commonly surveyed topics showed weaker alignment. This suggests that LLM performance depends on the availability of relevant patterns in training data---a consideration particularly important for subjective well-being constructs, which may be less explicitly represented in text corpora than political opinions. Indeed, the existing literature focuses predominantly on political attitudes; whether the silicon sampling paradigm extends to life satisfaction remains largely unexplored.

Cross-national extensions have revealed important limitations. \textcite{ma2024german} examined algorithmic fidelity in generating synthetic German public opinion, finding substantially reduced performance compared to American contexts. While the model captured some broad patterns, it failed to reproduce many Germany-specific relationships between demographics and opinions. This aligns with findings that American and English-language content predominates in LLM training data \parencite{li2024languageranker}. Similar performance variation might be expected across countries differing in language and cultural representation in training corpora.

The paradigm has attracted substantial criticism. \textcite{bisbee2024synthetic} provide a systematic critique, showing that while LLMs can match marginal distributions on many questions, they often fail to reproduce the correlational structure between variables, the patterns of which attitudes tend to co-occur within individuals. This matters because many social science analyses rely on relationships between variables rather than marginal distributions alone. They also documented troubling temporal instability: the same prompt yielded significantly different results over a three-month period, and minor wording changes produced substantial shifts in response distributions. These findings underscore the importance of systematic comparison across multiple measurement approaches.

\textcite{huang2025uncertainty} quantified these limitations by developing a framework for constructing confidence intervals when using LLM-simulated responses. Their analysis reveals that effective sample sizes are often below 100, and sometimes in single digits, even when nominal sample sizes are much larger. This occurs because LLM outputs lack the independence of human responses; the model's fixed parameters introduce correlations that reduce effective information content. When individual approaches yield imperfect approximations, combining information across multiple questionnaires or models might improve overall alignment---yet such ensemble strategies remain unexplored in the synthetic survey literature.

Despite these concerns, some evidence supports cautious optimism. In large-scale tests spanning 70 preregistered experiments, \textcite{hewitt2024predicting} found that GPT-4 predicted 91\% of variation in average treatment effects when adjusting for measurement error. However, they note that such simulations are best suited for exploratory research rather than definitive conclusions about human populations.

%-----------------------------------------------------------
\section{Questionnaire Design Effects in LLM Responses}
\label{sec:questionnaire-effects-llms}

Human survey respondents are sensitive to question wording, scale format, and context effects, phenomena extensively documented in survey methodology research. Whether LLMs exhibit similar sensitivities has important implications for their use as synthetic respondents.

\textcite{tjuatja2024llms} provide the most systematic examination of this question. They manipulated question order, response option ordering, scale direction, and other design features known to affect human responses, testing multiple models including GPT-3.5 and several open-source alternatives. Their findings reveal a complex picture: LLMs exhibit some human-like biases but not others, with patterns varying substantially across models and question types. For some design features, certain models showed sensitivity consistent with human patterns; for others, LLM responses were either insensitive to manipulations that affect humans or sensitive in inconsistent directions.

Most concerning, the authors found that LLMs can be highly sensitive to seemingly minor prompt variations, sometimes more sensitive than human respondents would be. Small phrasing changes that would be inconsequential for humans produced notable shifts in LLM outputs. This hypersensitivity raises questions about the robustness of LLM-generated survey data and suggests that comparing responses across distinct questionnaire formats measuring the same construct could reveal which measurement approaches produce consistent versus variable synthetic responses.

Research on human respondents documents that reversed items, where higher scores indicate lower construct levels, produce different psychometric properties than positively-worded items \parencite{suarez2018reversed, weijters2013reversed}. This ``reversed item bias'' reflects cognitive processing differences: respondents may fail to notice the reversal, apply different response strategies, or experience increased cognitive load when responding to negatively-worded items. The result is that scales containing reversed items sometimes show factor structures suggesting methodological artifacts rather than genuine construct dimensions.

Whether LLMs exhibit similar sensitivities to scale reversal remains underexplored. If LLMs struggle with reversed scales, either failing to recognize the reversal or responding inconsistently when scale direction changes, this would manifest as poorer approximation quality compared to standard formats. Similarly, metaphorical question framings such as the Cantril Ladder, which asks respondents to imagine their life position on a ladder from worst to best possible life, may elicit different processing than direct numerical ratings. Systematic comparison across these formats can provide empirical evidence on which measurement approaches LLMs handle well and which prove problematic. Yet studies typically employ a single questionnaire format, making it impossible to assess whether LLM performance varies with measurement approach.

This gap complements recent work by \textcite{wulff2025jinglejangle}, who addressed the jingle-jangle problem using semantic embeddings from LLMs. Their approach assesses whether instruments are semantically similar based on item content. While semantic analysis examines what instruments ask about, testing whether instruments elicit similar response patterns provides a behavioral rather than semantic comparison. Together, these approaches offer more comprehensive evidence about measurement relationships than either alone.

%-----------------------------------------------------------
\section{Demographic Conditioning and Persona Prompting}
\label{sec:demographic-conditioning}

How to condition LLMs on demographic information is a key methodological choice in generating synthetic survey responses. Researchers have developed various approaches, from simple attribute listing to elaborate narrative personas.

\textcite{lutz2025prompt} provide the most systematic evaluation, comparing prompting strategies across five open-source LLMs and 15 intersectional demographic groups. They examined direct role assignment (``You are a Black woman''), third-person framing, and interview-style formats where demographic information emerges through simulated dialogue. Interview-style prompts consistently outperformed other formats, producing responses with fewer stereotypical markers and better alignment with human data.

Surprisingly, \textcite{lutz2025prompt} found that smaller models sometimes outperformed larger ones on alignment metrics, challenging assumptions that larger models necessarily produce better synthetic survey data. This finding suggests that model selection for synthetic survey generation should be based on empirical validation rather than parameter count alone.

\textcite{hu2024persona} quantified the overall magnitude of persona effects, finding that persona variables account for less than 10\% of variance in model outputs for most tasks. This suggests that while demographic conditioning influences LLM responses, the effect may be smaller than commonly assumed. The optimal approach varies by demographic attribute: some characteristics (such as political orientation) are more reliably captured than others (such as income level). This variability implies that different demographic dimensions may show differential explanatory power in variance decomposition analyses.

However, persona-based approaches carry significant risks. \textcite{gupta2024bias} demonstrated that LLMs harbor deep-rooted biases that manifest when assigned personas. While models overtly reject stereotypes when asked directly (e.g., responding ``no'' to ``Are Black people less skilled at mathematics?''), they exhibit stereotypical presumptions when answering questions while adopting a persona. These biased assumptions appeared across 80\% of tested personas, with performance drops exceeding 70\% for some disadvantaged-group personas compared to majority-group personas. For subjective well-being measurement, this raises concerns that responses for low-income or poor-health personas may reflect stereotypical assumptions rather than actual patterns in human data.

\textcite{liu2024persona} extended this analysis to opinion generation, finding LLMs are 9.7\% less steerable toward incongruous personas---those combining statistically unusual trait combinations, such as political liberals supporting military spending. When prompted with such personas, LLMs sometimes generate stereotypical stances associated with one demographic trait rather than the target opinion. Incongruous combinations in life satisfaction contexts might include high-income individuals with poor health or low-income individuals reporting high satisfaction---cases where LLMs might default to stereotypical associations rather than capturing genuine human variation.

Cross-national applications face additional challenges. English-language content dominates LLM training corpora, with performance gaps of 5--15 percentage points common between English and other languages \parencite{li2024languageranker, gupta2025multilingual}. For survey simulation, this implies that cross-national comparisons may be confounded by differential model performance rather than genuine population differences. Comparing approximation quality across countries with different cultural contexts can help assess whether performance varies systematically with presumed training data representation.

In summary, the literature on LLMs as synthetic survey respondents reveals several gaps. First, most studies focus on political attitudes rather than subjective well-being constructs. Second, few studies systematically compare LLM performance across different questionnaire formats measuring the same construct. Third, ensemble approaches that combine responses across multiple instruments remain unexplored. This thesis addresses these gaps by comparing five life satisfaction questionnaires across multiple LLMs and countries, using distributional metrics to assess approximation quality and testing whether combining responses improves alignment with human data.

%-----------------------------------------------------------
% End of Chapter 3

