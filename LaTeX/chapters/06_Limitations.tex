% Chapter 6: Limitations

\chapter{Limitations}
\label{ch:limitations}

Several limitations of the research design, data, and analytical approach should be considered when interpreting the findings.

The study examined four countries (USA, Indonesia, Netherlands, Mexico), selected specifically for their non-normal life satisfaction distributions. While this selection enabled rigorous testing of whether LLMs can capture complex distributional shapes, it limits generalizability to countries with different response patterns. Countries with approximately normal distributions, excluded by design, may show different approximation dynamics. Relatedly, the study implicitly assumed that normally distributed responses might represent a ``default'' pattern that LLMs could approximate through random or uninformed guessing. However, it is equally plausible that LLMs have learned to produce skewed distributions as their default for life satisfaction questions, reflecting the response patterns prevalent in Western, Educated, Industrialized, Rich, and Democratic (WEIRD) populations that dominate their training corpora. This interpretation aligns with the observed pattern where the Netherlands showed the best approximation quality while Mexico showed the worst---the LLMs may be reproducing culturally specific response patterns rather than learning to match any arbitrary distribution. Within these countries, the demographic segmentation was limited to income level and health status, based on feature importance analysis identifying these as the strongest predictors of life satisfaction. Other demographic factors (age, gender, education, marital status, employment) were not systematically varied, and the findings regarding demographic effects may not extend to these unmeasured dimensions. Each demographic segment received 30 synthetic responses, chosen to balance computational costs with statistical power; larger samples might reveal finer-grained patterns or reduce sampling variability.

Beyond sample scope, model selection introduces constraints. Three LLMs were evaluated: LLaMA 3.1 8B, LLaMA 3.3 70B, and Qwen 2.5 72B, all open-weights models accessed through an academic cloud endpoint. Proprietary models (GPT-4, Claude) were not tested due to access constraints and cost considerations, and the finding that smaller models outperform larger ones may not generalize to proprietary architectures with different training procedures. Additionally, the models were queried with default temperature settings; systematic variation of generation parameters might yield different response distributions, but this parameter space was not explored. The study also employed a single prompting strategy (interview-based prompting), selected based on prior evidence of its effectiveness \parencite{lutz2025prompt}. Alternative strategies were tested during pilot work but not systematically compared in the main analysis, so the findings are conditional on this specific prompting approach.

Temporal factors present another source of uncertainty. The real survey data comes from World Values Survey Wave 7 (2017--2022), while the LLMs were trained on text corpora with different temporal coverage. This mismatch means that the models may have learned response patterns from earlier periods that do not reflect current population attitudes. Significant events such as the COVID-19 pandemic occurred during the WVS data collection period, potentially affecting life satisfaction distributions in ways not captured by the LLMs. Compounding this issue, the prompts did not specify any time frame for the synthetic respondents, leaving ambiguous whether generated responses reflect contemporary attitudes, historical patterns from the training data, or some mixture of both.

Finally, measurement and validation choices affect interpretation. The equipercentile equating procedure used to transform SWLS and OHQ scores to a common 1-10 scale involves assumptions about score correspondence that may introduce artifacts affecting comparisons across questionnaire types. More fundamentally, the evaluation focused on distributional alignment using Wasserstein distance and KS statistics, metrics that capture overall distributional similarity but do not assess whether synthetic responses are valid at the individual level. An LLM might produce plausible aggregate distributions through mechanisms quite different from human response processes. The study also treated WVS responses as ground truth, though survey responses themselves are subject to measurement error, social desirability bias, and other validity threats. Moreover, the same WVS participants might have responded differently had they been administered the alternative scales (SWLS, OHQ, Cantril); the comparison thus evaluates alignment with one particular operationalization of life satisfaction rather than with participants' ``true'' satisfaction levels across measurement approaches. The evaluation therefore assesses alignment with measured responses rather than with ``true'' underlying life satisfaction levels. Whether the patterns observed, particularly the advantages of certain questionnaire formats and the challenges with reversed scales, extend to other survey constructs such as political attitudes, health behaviors, or consumer preferences remains untested.
