% Chapter 7: Conclusion

\chapter{Conclusion}
\label{ch:conclusion}

This thesis investigated whether large language models can generate synthetic survey responses that approximate real human life satisfaction distributions. Through systematic comparison of LLM-generated responses with World Values Survey data across four countries, five questionnaire formats, and three models, the research addressed three questions about approximation quality, questionnaire design effects, and ensemble improvement strategies.

Regarding the first research question, to what extent LLMs can approximate human responses, the findings indicate moderate approximation with substantial variation across conditions. The best configurations achieved near-perfect alignment (Wasserstein distance of 0.16 for the Netherlands with LLaMA 3.1 8B), while poor configurations showed large discrepancies (W > 3.0). Mean performance across all 540 segment-level comparisons was W = 2.09. A counterintuitive finding emerged: the smallest model (LLaMA 3.1 8B) consistently outperformed larger models, achieving an 83\% win rate across segment-questionnaire combinations. Health status emerged as the most influential demographic factor, explaining 11\% of variance in approximation quality, while income level showed negligible influence.

The second research question, whether certain scales are better approximated, received a clear affirmative answer. The Original WVS and SWLS formats achieved the best performance (mean W $\approx$ 1.0), while the Reverse Scale performed substantially worse (mean W = 2.53), 166\% higher than the Original WVS. This indicates that LLMs struggle to correctly process reversed scale orientations. Multi-item scales showed more consistent performance across demographic contexts than single-item measures.

The third research question, whether ensemble approaches can improve alignment, also received strong support. Both KS-based optimal weighting and equal-weight averaging achieved mean Wasserstein distances of approximately 0.70, a 26\% improvement over the best individual questionnaire. The near-identical performance of both weighting schemes suggests that equal-weight averaging is preferable for practical applications due to its simplicity.

These findings carry several implications for researchers considering LLM-generated survey data. Synthetic data can achieve reasonable distributional alignment under favorable conditions, but quality varies substantially across demographic groups, questionnaire formats, and target populations. Model selection matters, but not in the expected direction: smaller models may outperform larger ones, suggesting computational resources could be better allocated to generating more responses rather than using larger architectures. Questionnaire design requires careful consideration: reversed scales should be avoided, and combining responses across multiple formats through ensemble methods provides measurable improvement. Certain demographic contexts pose greater challenges, with health status influencing approximation quality more than income.

Several directions for future research emerge from this work. Extending the evaluation to other survey constructs would test whether these patterns generalize beyond life satisfaction. Systematic comparison of prompting strategies could identify approaches that further improve approximation quality. Evaluation of proprietary models would clarify whether the smaller-is-better finding reflects general principles or specific characteristics of open-weights architectures. More fundamentally, the mechanisms underlying LLM survey simulation remain unclear; why smaller models outperform larger ones and why reversed scales pose such difficulty are questions that could inform both model development and survey design. Finally, practical deployment of synthetic survey data requires validation frameworks and guidelines for when synthetic data is sufficient for specific research purposes.
